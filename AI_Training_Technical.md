# AI Training Technical Documentation

This document provides a detailed technical overview of the AI training system and the structure of the trained models within the ConnectX project.

## 1. Training Philosophy and Objectives

The primary goal of the training system is to create a knowledge base (model) that allows the AI to make optimal or near-optimal moves for various board states without performing real-time minimax calculations during gameplay. This shifts the computational burden from gameplay to an offline training phase, enabling faster AI responses in-game.

## 2. Board State Generation

The `train_ai.py` script is responsible for generating the board states that the AI will be trained on. Two primary modes of generation are supported:

### 2.1 Fixed-Depth Generation

When a specific `depth` is provided (e.g., `./launch.sh train 3`), the system generates all possible board configurations that result from exactly `depth` moves. For a Connect 4 board with 7 columns, this results in `7^depth` potential move sequences. Each resulting board state is then evaluated by the AI.

* **Methodology:** Recursive depth-first search. At each level of recursion, a piece is dropped into each valid column, and the function calls itself for the next move until the `depth` is reached.
* **Pruning:** If a game state becomes terminal (win or draw) before the specified `depth` is reached, no further moves are generated from that branch. However, the terminal state itself is included for evaluation.
* **Scale:** The number of combinations grows exponentially `7^depth`. For `depth=6`, this is **117,649** combinations.

### 2.2 Full Game Traversal (BFS)

When no `depth` is specified (e.g., `./launch.sh train`), the system performs a Breadth-First Search (BFS) to explore all unique, reachable board states from an empty board until every possible game path leads to a terminal state (win or draw).

* **Methodology:** A queue (`collections.deque`) is used to manage board states to visit. A `set` (`visited_states`) stores previously encountered board states (as hashable tuples) to prevent redundant computation and infinite loops.
* **Terminal State Handling:** Once a board state is identified as terminal (win or draw), it is added to the training set, but no further moves are generated from it.
* **Computational Intensity:** This mode explores the vast Connect 4 game tree, which contains over 4 trillion reachable states. This is extremely computationally intensive and requires significant time and memory resources.

## 3. Model Structure

The trained model is stored as a Python `pickle` file (`.pkl`) and contains a dictionary. The structure of this dictionary (`self.model` in `AITrainer`) is as follows:

```python
{
    board_key_1: {
        player_1_piece_id: best_move_column_for_player_1,
        player_2_piece_id: best_move_column_for_player_2
    },
    board_key_2: {
        player_1_piece_id: best_move_column_for_player_1,
        player_2_piece_id: best_move_column_for_player_2
    },
    # ... more board states
}
```

* **`board_key`**: A hashable representation of the Connect 4 board state. This is generated by converting the NumPy array board into a tuple of tuples (`tuple(map(tuple, board.astype(int)))`). This allows it to be used as a dictionary key.
* **`player_piece_id`**: The integer ID of the player (e.g., `1` for Player 1, `2` for Player 2) for whom the `best_move_column` was calculated in that specific board state.
* **`best_move_column_for_player`**: The optimal column (0-6) that the specified player should choose from that `board_key` to maximize their chances of winning, as determined by the minimax algorithm during training.

## 4. Training Process

### 4.1 AI Evaluation during Training

During training, for each generated board state, an `AI` instance (with `difficulty="HARD"`) is used to determine the `best_move` using the minimax algorithm. The `max_depth` for this internal AI evaluation is set to the training `depth` (for fixed-depth training) or a default of 3 (for full game traversal, as deeper real-time evaluation is too costly).

### 4.2 Multithreading

To accelerate the training process, board evaluations are performed concurrently using `concurrent.futures.ThreadPoolExecutor`. The number of worker threads is dynamically set to approximately 75% of the available CPU cores (`os.cpu_count() * 0.75`), with a minimum of 1 thread.

* **Thread Safety:** Each worker thread receives its own independent `AI` instance for evaluating a board position. This prevents race conditions and ensures thread safety, as `AI` instances maintain internal state during minimax calculations.
* **Progress Tracking:** `tqdm` provides a progress bar, which for fixed-depth training indicates `Y/Total_Combinations`, and for full game traversal, indicates `Y/Total_States_Discovered_So_Far`.

### 4.3 Error Handling

Exceptions during individual board evaluations within threads are caught and logged, allowing the training process to continue without crashing due to an isolated issue. The script also includes warnings for extremely high depths due to their significant computational requirements.

## 5. Model Saving and Loading

### 5.1 Saving

Upon completion of training, the `self.model` dictionary is serialized using Python's `pickle` module and saved to the `models/` directory. The filename follows a consistent convention:

* `model_depth_[depth]_date_[YYYY-MM-DD_HH_MM_SS].pkl` (for fixed-depth training)
* `model_full_game_date_[YYYY-MM-DD_HH_MM_SS].pkl` (for full game traversal)

### 5.2 Loading and Usage

In `main.py`, the `AI` class can be initialized with a `model_file` path. If a model is loaded, the `AI.get_move()` method first attempts to retrieve the best move directly from the loaded model for the current board state. If the board state is not found in the model (e.g., if the model was trained on a shallower depth), the AI gracefully falls back to its regular minimax evaluation logic (Easy/Medium/Hard difficulty). This provides a robust fallback mechanism for unseen states.
